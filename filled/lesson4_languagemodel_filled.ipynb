{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Demo\n",
    "\n",
    "Based on this demo: http://nlpforhackers.io/language-models/\n",
    "\n",
    "### Import modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\varsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.corpus import reuters, movie_reviews, shakespeare\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose a corpus: reuters, movie_reviews or shakespeare\n",
    "corpus = movie_reviews\n",
    "\n",
    "if corpus==shakespeare:\n",
    "    shakespeare_text = ''.join([''.join(corpus.xml(fileid).itertext()) for fileid in corpus.fileids()])\n",
    "    words = word_tokenize(shakespeare_text)\n",
    "    sents = [word_tokenize(sent) for sent in sent_tokenize(shakespeare_text)]\n",
    "else:    \n",
    "    words = corpus.words()\n",
    "    sents = corpus.sents()\n",
    "\n",
    "# Lowercase everything\n",
    "words = [w.lower() for w in words]\n",
    "sents = [[w.lower() for w in sent] for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram language model\n",
    "\n",
    "In this section, we will construct a language model based on unigrams (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in corpus:  1583820\n",
      "\n",
      "Top 10 most common words: \n",
      ", 77717\n",
      "the 76529\n",
      ". 65876\n",
      "a 38106\n",
      "and 35576\n",
      "of 34123\n",
      "to 31937\n",
      "' 30585\n",
      "is 25195\n",
      "in 21822\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1. Fill in the blanks.\n",
    "\n",
    "\n",
    "# Step 1: Make a Counter from the list of words and call it \"unigram_counts\" (remember, this is easy to do!)\n",
    "unigram_counts = Counter(words)\n",
    "\n",
    "# Step 2: Get the total number of words and assign it to \"total_count\"\n",
    "total_count = len(words)\n",
    "\n",
    "\n",
    "print(\"Total number of words in corpus: \", total_count)\n",
    "\n",
    "# Print 10 most common words\n",
    "print(\"\\nTop 10 most common words: \")\n",
    "for (word, count) in unigram_counts.most_common(n=10):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities sum to:  1.0000000000003604\n",
      "\n",
      "Top 10 most common words: \n",
      ", 0.04907\n",
      "the 0.04832\n",
      ". 0.04159\n",
      "a 0.02406\n",
      "and 0.02246\n",
      "of 0.02154\n",
      "to 0.02016\n",
      "' 0.01931\n",
      "is 0.01591\n",
      "in 0.01378\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2. Fill in the blanks.\n",
    "\n",
    "# We have the Counter unigram_counts, which maps each word to its count.\n",
    "# We want to construct the Counter unigram_probs, which maps each word to its probability.\n",
    "\n",
    "\n",
    "# Step 1: create an empty Counter called unigram_probs.\n",
    "unigram_probs = Counter()\n",
    "\n",
    "\n",
    "# Step 2: using a for-loop over unigram_counts, (this will iterate over the keys i.e. words)\n",
    "# calculate the appropriate fraction, and add the word -> fraction pair to unigram_probs.\n",
    "# Remember about integer division!\n",
    "for word in unigram_counts:\n",
    "    unigram_probs[word] = unigram_counts[word] / float(total_count)\n",
    "\n",
    "\n",
    "# Check the probabilities add up to 1\n",
    "print(\"Probabilities sum to: \", sum(unigram_probs.values()))\n",
    "\n",
    "# Print 10 most common words\n",
    "print(\"\\nTop 10 most common words: \")\n",
    "for (word, count) in unigram_probs.most_common(n=10):\n",
    "    print(word, \"%.5f\"%count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048319253450518365\n"
     ]
    }
   ],
   "source": [
    "# Print the probability of word \"the\", then try some other words.\n",
    "print(unigram_probs['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to a he casting , ineffective assurance the poorer fidel ditzy is so be next tower live we rachel his does trust does the an of \" a of and - details surprises robert charming and , mostly , an i from or goes is dam writing act , ' gets becomes . whole that the the quality camp work but the . he same was says closer flynt he , hope comprehensive to , by they ? and the computer bring of . playing line as who on . one movies look skills sympathetic , niro to we like\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 words of language using the unigram model.\n",
    "# Run this cell several times!\n",
    "\n",
    "text = [] # will be a list of generated words\n",
    "\n",
    "for _ in range(100):\n",
    "    r = random.random() # random number in [0,1]\n",
    "    \n",
    "    # Find the word whose \"interval\" contains r\n",
    "    accumulator = .0\n",
    "    for word, freq in unigram_probs.items():\n",
    "        accumulator += freq\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram language model\n",
    "\n",
    "In this section, we'll build a language model based on bigrams (pairs of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count how often each bigram occurs.\n",
    "\n",
    "# bigram_counts is a dictionary that maps w1 to a dictionary mapping w2 to the count for (w1, w2)\n",
    "bigram_counts = defaultdict(lambda: Counter())\n",
    "\n",
    "for sentence in sents:\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        bigram_counts[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8621\n"
     ]
    }
   ],
   "source": [
    "# Print how often the bigram \"of the\" occurs. Try some other words following \"of\".\n",
    "print(bigram_counts['of']['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the bigram counts to bigram probabilities\n",
    "bigram_probs = defaultdict(lambda: Counter())\n",
    "for w1 in bigram_counts:\n",
    "    total_count = float(sum(bigram_counts[w1].values()))\n",
    "    bigram_probs[w1] = Counter({w2: c/total_count for w2,c in bigram_counts[w1].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25264484365384055\n"
     ]
    }
   ],
   "source": [
    "# Print the probability that 'the' follows 'of'\n",
    "print(bigram_probs['of']['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' 0.30588\n",
      "me 0.17176\n",
      "the 0.08000\n",
      "alone 0.05882\n",
      "him 0.05412\n",
      "you 0.03294\n",
      "us 0.02353\n",
      "down 0.02118\n",
      "that 0.01882\n",
      "go 0.01882\n"
     ]
    }
   ],
   "source": [
    "# Print the top ten tokens most likely to follow 'fair', along with their probabilities.\n",
    "# Try some other words.\n",
    "prob_dist = bigram_probs['let']\n",
    "for word,prob in prob_dist.most_common(10):\n",
    "    print(word,\"%.5f\"%prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspector gadget .\n"
     ]
    }
   ],
   "source": [
    "# Generate text with bigram model.\n",
    "# Run this cell several times!\n",
    "\n",
    "text = [None] # You can put your own starting word in here\n",
    "sentence_finished = False\n",
    "\n",
    "# Generate words until a None is generated\n",
    "while not sentence_finished:\n",
    "    r = random.random() # random number in [0,1]\n",
    "    accumulator = .0\n",
    "    latest_token = text[-1]\n",
    "    prob_dist = bigram_probs[latest_token] # prob dist of what token comes next\n",
    "    \n",
    "    # Find the word whose \"interval\" contains the random number r.\n",
    "    for word,p in prob_dist.items():\n",
    "        accumulator += p \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-1] == None:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the bigram text compare to the unigram text?\n",
    "\n",
    "### Trigram language model\n",
    "\n",
    "In this section, we'll build a language model based on trigrams (triples of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count how often each trigram occurs.\n",
    "\n",
    "# trigram_counts maps (w1, w2) to a dictionary mapping w3 to the count for (w1, w2, w3)\n",
    "trigram_counts = defaultdict(lambda: Counter())\n",
    "\n",
    "for sentence in sents:\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        trigram_counts[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# Print how often the trigram \"I am not\" occurs. Try some other trigrams.\n",
    "print(trigram_counts[('i', 'am')]['not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the trigram counts to trigram probabilities\n",
    "trigram_probs = defaultdict(lambda: Counter())\n",
    "for w1_w2 in trigram_counts:\n",
    "    total_count = float(sum(trigram_counts[w1_w2].values()))\n",
    "    trigram_probs[w1_w2] = Counter({w3: c/total_count for w3,c in trigram_counts[w1_w2].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16363636363636364\n"
     ]
    }
   ],
   "source": [
    "# Print the probability that 'not' follows 'i am'. Try some other combinations.\n",
    "print(trigram_probs[('i', 'am')]['not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not 0.16364\n",
      "a 0.07273\n",
      "sure 0.07273\n",
      "the 0.03030\n",
      "willing 0.02424\n",
      "going 0.02424\n",
      ", 0.02424\n",
      "of 0.01818\n",
      "glad 0.01818\n",
      "thinking 0.01212\n"
     ]
    }
   ],
   "source": [
    "# Print the top ten tokens most likely to follow 'i am', along with their probabilities.\n",
    "# Try some other pairs of words.\n",
    "prob_dist = trigram_probs[('i', 'am')]\n",
    "for word,prob in prob_dist.most_common(10):\n",
    "    print(word,\"%.5f\"%prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my advice , director brett ratner ( the title and his cat .\n"
     ]
    }
   ],
   "source": [
    "# Generate text with trigram model.\n",
    "# Run this cell several times!\n",
    "\n",
    "text = [None, None] # You can put your own first two words in here\n",
    "\n",
    "sentence_finished = False\n",
    "\n",
    "# Generate words until two consecutive Nones are generated\n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "    latest_bigram = tuple(text[-2:])\n",
    "    prob_dist = trigram_probs[latest_bigram] # prob dist of what token comes next\n",
    "    \n",
    "    for word,p in prob_dist.items():\n",
    "        accumulator += p \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How does the trigram text compare to the bigram text?\n",
    "\n",
    "## Extension exercise\n",
    "\n",
    "N-gram language models can encounter the *sparsity problem*, especially if the data is small.\n",
    "\n",
    "Suppose you train a trigram language model on a small amount of data (let's say the text of *The Hunger Games*), then use the language model to generate text.\n",
    "\n",
    "On each step, you take the last two generated words (e.g. \"may the\") and lookup the probability distribution of what word is most likely to come next. But if your training data is small, perhaps there is only one example of the bigram \"may the\" in the training data (e.g. \"may the odds be ever in your favor\" in *The Hunger Games*). In that case, the next word will be *odds* with probability 1. This means that your language model always says \"odds\" after saying \"may the\".\n",
    "\n",
    "1. Is the sparsity problem worse for unigram language models, bigram language models, trigram language models, or n-gram language models for n>3?\n",
    "2. How might you fix this problem? \n",
    "3. How might you fix this problem without access to more training data?\n",
    "\n",
    "Try altering either the bigram or the trigram language model with your solution to question 3."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
