{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this every time you open the spreadsheet\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import Counter\n",
    "import lib\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise: Add bigram capabilities to the classifier!\n",
    "\n",
    "So far our Naive Bayes classifier scores an Average F1 score of 66.9% on the test set.\n",
    "Let's see if we can improve on that by incorporating bigrams!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_bigrams(tweet):\n",
    "    # Currently, tweet has an attribute called tweet.tokenList which is a list of tokens.\n",
    "    # You want to add a new attribute to tweet called tweet.bigramList which is a list of bigrams.\n",
    "    # Each bigram should be a pair of strings. You can define the bigram like this: bigram = (token1, token2).\n",
    "    # In Python, this is called a tuple. You can read more about tuples here: https://www.programiz.com/python-programming/tuple\n",
    "\n",
    "    ##### YOUR CODE STARTS HERE #####\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE #####\n",
    "\n",
    "\n",
    "tweets, test_tweets = lib.read_data()\n",
    "for tweet in tweets+test_tweets:\n",
    "    add_bigrams(tweet)\n",
    "print(\"Checking if bigrams are correct...\")\n",
    "for tweet in tweets+test_tweets:\n",
    "    assert tweet._bigramList==tweet.bigramList, \"Error in your implementation of the bigram list!\"\n",
    "print(\"Bigrams are correct.\\n\")\n",
    "\n",
    "prior_probs, token_probs = lib.learn_nb(tweets)\n",
    "predictions = [(tweet, lib.classify_nb(tweet, prior_probs, token_probs)) for tweet in test_tweets]\n",
    "lib.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the classifier and get evaluation score\n",
    "\n",
    "This notebook uses our implementation of the Naive Bayes classifier, but it's very similar to what you implemented yesterday. If you're interested in the details, take a look at the `learn_nb` and `classify_nb` functions in `lib.py` in the `sailors2017` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets, test_tweets = lib.read_data()\n",
    "prior_probs, token_probs = lib.learn_nb(tweets)\n",
    "predictions = [(tweet, lib.classify_nb(tweet, prior_probs, token_probs)) for tweet in test_tweets]\n",
    "lib.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Classifier\n",
    "\n",
    "After implementing and training a classifier, you often want to inspect what kind of things it has learned and how it is making predictions on individual examples. This can help you make sure that you implemented everything correctly and it might give you ideas on how to further improve the classifier.\n",
    "\n",
    "### Most discriminative words\n",
    "\n",
    "Let's first look again at the most discriminative words for each category, i.e. the words that maximize P(category|word), for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lib.most_discriminative(tweets, token_probs, prior_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These five lists show you which words are most predictive of the five categories. For example, the word _bottled_ is a very strong indicator that the tweet is about water or the word _canned_ is a very strong indicator that the tweet is about food.\n",
    "\n",
    "Many of you used several of these words in your rule-based classifiers in week 1. It's reassuring (and exciting!) to see that the Naive Bayes classifier learned that these words are good indicators of the categories as well.\n",
    "\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "Another useful type of visualization is a so-called confusion matrix. A confusion matrix shows you for each true category _c_ how many of the tweets in _c_ were classified into the five different categories. (In this way it tells you which categories are \"confused\" for others by the classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lib.show_confusion_matrix(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix, the **rows** correspond to the **true category** and the **columns** correspond to the **predicted category**.\n",
    "\n",
    "For example, this matrix shows you that of all the 79 tweets in the category _None_, 13 were incorrectly classified as _Energy_, 3 as _Food_, and 1 as _Medical_. 62 of them were actually correctly classified as _None_.\n",
    "\n",
    "### Visualizing individual tweets\n",
    "\n",
    "It can also be really useful to visualize the probabilities of each token in an individual tweet. This can help you understand why a classifier made a correct or wrong prediction. We've implemented a visualization for you so that you can use to inspect how the classifier works on individual tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following code visualizes a random tweet from the test data. \n",
    "# Hover your mouse over the words!\n",
    "\n",
    "random_tweet = random.choice(test_tweets)\n",
    "lib.visualize_tweet(random_tweet, prior_probs, token_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color of each word tells you for which category $P(\\text{token} \\mid \\text{category})$ is the highest. When you move the mouse over a word, it shows you the actual values of $P(\\text{token} \\mid \\text{category})$ for each category that the classifier uses to make its predictions.\n",
    "\n",
    "You can also have the classifier make a prediction on your own tweets. Change the text in `my_tweet` below and run the cell below to see what the classifier would predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tweet = \"I urgently need some bottled water.\"\n",
    "\n",
    "lib.visualize_tweet(lib.Tweet(my_tweet, \"?\", \"\"), prior_probs, token_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis: Figuring out remaining errors\n",
    "\n",
    "Often, one wants to know in which scenarios a classifier makes mistakes. This can be really useful when you want to improve your classifier.\n",
    "\n",
    "In this exercise, try to break the Naive Bayes classifier. Use the cell above and try to come up with a tweet which should be classified as _Food_ but which is assigned a different category. Once you find such a tweet, use the visualization to figure out why the classifier gets this example wrong.\n",
    "\n",
    "Repeat this exercise for all the other categories. Based on your observations, do you have any ideas on how to further improve the classifier?\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
